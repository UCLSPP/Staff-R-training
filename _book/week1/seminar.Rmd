# R training for SPP staff (session 1)
#### Javier Sajuria
#### 2 December 2015

## Introduction to R 

### Scalar objects

```{r}
# Create a numeric and a character variable
a <- 5 
class(a) # a is a numeric variable
a
b <- "Yay stats class"
class(b) # b is a string variable
b
```

Save your script, and re-open it to make sure your changes are still there.

### Vectors and subsetting

```{r}
# Create a vector
my.vector <- c(10,-7,99,34,0,-5) # a vector
my.vector
length(my.vector) # how many elements?
# subsetting
my.vector[1] # 1st vector element
my.vector[-1] # all elements but the 1st
my.vector[2:4] # the 2nd to the 4th elements
my.vector[c(2,5)] # 2nd and 5th element
my.vector[length(my.vector)] # the last element

# delete variable 'a' from workspace
rm(a)
# delete everything from workspace
rm(list=ls())
```


### Matrices
```{r}
# create a matrix
# type help("matrix") into the console and press ENTER
# read Description, Usage and Arguments
my.matrix1 <- matrix(data = c(1,2,30,40,500,600), nrow = 3, ncol = 2, byrow = TRUE,
                     dimnames = NULL)
my.matrix2 <- matrix(data = c(1,2,30,40,500,600), nrow = 2, ncol = 3, byrow = FALSE)
# How are the matrices different?
my.matrix1
my.matrix2

# subsetting a matrix
my.matrix1[1,2] # element in row 1 and column 2
my.matrix1[2,1] # element in row 2 and column 1
my.matrix1[,1] # 1st column only
my.matrix1[1:2,] # rows 1 to 2
my.matrix1[c(1,3),] # rows 1 and 3 
```


### Installing and loading packages

Packages are user-generated pieces of code that expand the basic options of R. R is a very flexible language, that allows to go way beyond the base options.

```{r, eval=FALSE}
install.packages("texreg") # Creates tables both in ASCII text, LaTeX or Word, similar to outreg
install.packages("lmtest") # Provides different tests of linear models
install.packages("readxl") # Opens and writes Excel files
install.packages("sandwich") # Calculates heteroskedasticity consistent SEs
install.packages("car") # General functions to run regressions and manage data
install.packages("plm") # Panel data models
install.packages("dplyr") # General data manipulation
install.packages("tidyr") # Further data manipulations
install.packages("ggplot2") # Advanced graphical machine
install.packages("effects")
```

In some cases, we want to install previous versions of packages. In this case, we will install Zelig, a comprehensive package that we will use to estimate and plot predicted probabilities. We will install the same version we are using to teach the MSc students:

```{r, eval=FALSE}

install.packages("https://cran.r-project.org/src/contrib/Archive/Zelig/Zelig_4.2-1.tar.gz", 
                 repos=NULL, 
                 type="source")

```

Once packages are installed, they need to be loaded. The reason is that some packages have overlapping functions with others, so we usually care about the order in which we load them

```{r}
library(foreign) ## comes with the basic installation and allows us to open files in other formats such as Stata, SPSS or SAS
library(car)
library(readxl) 
library(texreg)
library(Zelig)
library(sandwich)
library(plm)
library(effects)
library(ggplot2)
library(tidyr)
library(lmtest)
library(dplyr)
```



To set up your working directory, you need to use the `setwd()` function. E.g.:

```
setwd("~/R seminar")
```

## Downloading data and descriptive stats

One traditional format of data as CSV. These files do not contain any metadata, but they are usually compatible with any statistical software. The way to load a dataset is assigning it to an object (of class `dataframe`). For CSVs, we use `read.csv()`:

```{r}
# load the Polity IV dataset
my.data <- read.csv("http://uclspp.github.io/PUBLG100/data/polity.csv")

# View(my.data) # opens a window with the data set
head(my.data) # retrieves the first 6 observations
head(my.data, n=10) # you can manually set up the amount of observations shown

tail(my.data) # retrieves the last 6 observations

levels(my.data$country) # levels displays levels of a factor variable

# we drop all oberservations which are not from 1946
my.data <- my.data[my.data$year==1946,]
head(my.data)

summary(my.data$polity2) # descriptive statistics of polity variable

# now lets check if western countries were more democratic than the other countries in 1946
table(my.data$nato, my.data$polity2)
# descriptive summary stats of polity variable by nato membership 
summary(my.data$polity2[my.data$nato==0]) # not in nato
summary(my.data$polity2[my.data$nato==1]) # nato member

## illustration
boxplot(my.data$polity2 ~ as.factor(my.data$nato),
        frame = FALSE,
        main = "Polity IV Scores of NATO founders vs others in 1946",
        xlab = "NATO member",
        ylab = "Polity Score")

```


We can use the `read_excel` function from the `readxl` package to open Excel files. However, unlike with what we did for CSV files, the `read_excel` function does not directly download files, so we need to save them into our working directory. 

<a href="http://uclspp.github.io/PUBLG100/data/hsb2.xlsx" type="button" class="btn btn-success">Download 'High School and Beyond' Dataset</a>

Make sure you save it into your working directory and then run:

```{r}
student_data <- read_excel("hsb2.xlsx")

head(student_data)

```

To open Stata files, we can use the `read.dta()` function:

```{r}
world.data <- read.dta("http://uclspp.github.io/PUBLG100/data/QoG2012.dta")
head(world.data)
```

We will start looking at the `student_data` dataset:

```{r}

mean(student_data$science) # Mean 
sd(student_data$science) # Standard deviation
sd(student_data$science)^2 # Variance
median(student_data$science) # Median
range(student_data$science) # Minimum and Maximum value

summary(student_data$science)

hist(student_data$science, main = "Histogram of Science Scores", xlab = "Science Score")
```


Now let's suppose we wanted to find out the highest score in science and also wanted to see the ID of the student who received the highest score.

The [`max()`](http://bit.ly/R_extremes) function tells us the highest score, and [`which.max()`](http://bit.ly/R_which_min) tells us the row number of the student who received it.

```{r}
max(student_data$science)
which.max(student_data$science)
```

In addition to the median, the 25th percentile (also known as the lower quartile) and 75th percentile (or the upper quartile) are commonly used to describe the distribution of values in a number of fields including standardized test scores, income and wealth statistics and healthcare measurements such as a baby's birth weight or a child's height compared to their respective peer group. 

We can calculate percentiles using the [`quantile()`](http://bit.ly/R_quantile) function in R. For example, if we wanted to see the science scores in the 25th, 50th and 75th percentiles, we would call the [`quantile()`](http://bit.ly/R_quantile) function with `c(0.25, 0.5, 0.75)` as the second argument.

```{r}
quantile(student_data$science, c(0.25, 0.5, 0.75))
```

Obtaining the mode is slightly more difficult, but it takes only a couple of extra steps. We will use a categorical variable, such as `race` from the `student_data` file.

#### Factor Variables


The High School and Beyond dataset that we've been using contains categorical variable such as race, gender and socioeconomic status that are coded as numeric data and must be converted to factor variables. 

We'll use the following code book to create categorical variables for gender, race, and socioeconomic status.

|Categorical Variable|New Factor Variable|Levels|
|-|-|-|
|female|gender|0 - Male <br> 1 - Female|
|ses|socioeconomic_status|1 - Low <br> 2 - Middle <br> 3 - High|
|race|racial_group|1 - Black <br> 2- Asian <br> 3 - Hispanic <br> 4 - White|

We can convert categorical variables to factor variables using the [`factor()`](http://bit.ly/R_factor) function. The [`factor()`](http://bit.ly/R_factor) function needs the categorical variable and the distinct labels for each category (such as "Male", "Female") as the two arguments for creating factor variables.

```{r}
student_data$sex <- factor(student_data$female, labels = c("Male", "Female")) 
student_data$socioeconomic_status <- factor(student_data$ses, labels = c("Low", "Middle", "High")) 
student_data$racial_group <- factor(student_data$race, labels = c("Black", "Asian", "Hispanic", "White")) 
```

Based on this, let's get the mode

```{r}
race_table <- table(student_data$racial_group) # This tabulates the frequency per value
race_table
sort(race_table, decreasing = TRUE)
```


Now that we've created factor variables for our categorical data, we can run crosstabs on these newly created factor variables . We know that our dataset has `r nrow(student_data)` observations, but let's see how many are male students and how many are female students.

```{r}
table(student_data$sex)
```

Next, let's see how the different socioeconomic groups (Low, Middle, High) are represented in our dataset.

```{r}
table(student_data$socioeconomic_status)
```

Finally, we can run two-way crosstabs to see how the different racial groups are distributed over the three socioeconomic levels.

```{r}
table(student_data$socioeconomic_status, student_data$racial_group)
```

### Visualizing Data

Let's move on to some visualizations starting with a bar chart of socioeconomic status and let's take advantage of the factor variable we created above. Since `socioeconomic_status` is a factor variable, R automatically knows how to draw bars for each factor and label them accordingly.

```{r}
# bar charts
barplot(table(student_data$socioeconomic_status))
```

In addition to being able to correctly group and label a bar chart with distinct categories, factor variables can also be used to create box plots automatically with the [`plot()`](http://bit.ly/R_plot) function. The [`plot()`](http://bit.ly/R_plot) function understands that we are interested in a box plot when we pass it a factor variable for the x-axis.

We can use the [`par()`](http://bit.ly/R_par) function to change graphical parameters and instruct R to create a figure with 1 row and 3 columns using the `mfrow=c(1,3)` option. Once the graphical parameters are set, the three plots for gender, race and socioeconomic status variables will be created side-by-side in a single figure.

We would like to rotate the x-axis labels 90 degrees so that they are perpendicular to the axis. Most graphics functions in R support a label style `las` option for rotating axis labels. The `las` option can take these 4 values:

|Value|Axis Labels|
|-|-|-|
|0|Always parallel to the axis [default]|
|1|Always horizontal|
|2|Always perpendicular to the axis|
|3|Always vertical|

To rorate the axis labels 90 degrees, we'll use `las = 2` when calling the [`plot()`](http://bit.ly/R_plot) function.

```{r}
# science score by gender, race and socioeconomic status
par(mfrow=c(1,3))

# categorical variables are plotted as boxplots
plot(student_data$sex, student_data$science, main = "Gender", las = 2)
plot(student_data$racial_group, student_data$science, main = "Race", las = 2)
plot(student_data$socioeconomic_status, student_data$science, main = "Socioeconomic Status", las = 2)
```

Now let's see if we can visually examine the relationship between science and math scores using a scatter plot. Before we call the [`plot()`](http://bit.ly/R_plot) function we need to reset the graphical parameters to make sure that our figure only contains a single plot by calling the [`par()`](http://bit.ly/R_par) function and using the `mfrow=c(1,1)` option.

```{r}
par(mfrow=c(1,1))
plot(student_data$math, student_data$science)
```

### The `apply()` Function

Suppose we wanted to create a new variable called `english` which represented the average of each student's reading and writing scores. We could call the [`mean()`](http://bit.ly/R_mean) function manually for each student and update the `english` variable but that would be inefficient and extremely time consuming. Fortunately, R provide us built-in support for tasks like this with the [`apply()`](http://bit.ly/R_apply) function. The appropriately named [`apply()`](http://bit.ly/R_apply) function allows us to 'apply' any function to either the rows or the columns of a matrix or a data frame in a single call.

Here is a list of arguments the [`apply()`](http://bit.ly/R_apply) function expects and their descriptions:

```
apply(x, margin, function)
```

|Argument|Description|
|-|-|
|`x`|The first argument is the dataset that we want the apply function to operate on. Since we're averaging over reading and writing scores, we select only the `read` and `write` columns.|
|`margin`|The second argument tells [`apply()`](http://bit.ly/R_apply) to either apply the function row-by-row (1) or column-by-column (2). We need to apply the mean function to each row by specifying the second argument as `1`.|
|`function`|The third argument is simply the function to be applied to each row, or `mean` in our case.|

Let's take a look at [`apply()`](http://bit.ly/R_apply) in action:

```{r}
student_data$english <- apply(student_data[c("read", "write")], 1, mean)
```

Let's use [`head()`](http://bit.ly/R_head) to inspect our dataset with the english scores.

```{r}
head(student_data)
```

### Distributions in R

One of the default packages in R called `stats` provides a number of functions for drawing random samples from a distribution. 

The [`rnorm()`](https://bit.ly/R_normal) function, for example, can be used to draw from a normal distribution.

```{r}
normal_dist <- rnorm(1000, mean = 0, sd = 1)
head(normal_dist)
hist(normal_dist)
```

The [`runif()`](http://bit.ly/R_uniform) function can be used to draw from a uniform distribution. For example, we can simulate rolling a 6-sided die with the following code.

```{r}
num_rolls <- 10 # number of times to roll the dice
rolls <- as.integer(runif(num_rolls, min = 1, max = 7))
rolls
```

## Regression

The basic function for fitting linear models is `lm()`. It is always recommended to create an object with the results from the `lm()` function and then summarise it. The way in which it works is:

```
model1 <- lm(DV ~ IV1 + IV2, data)
summarise(model1)
```

The arguments are:

|Argument|Description|
|-|-|
|Formula| `DV ~ IV`|
|data| The dataset where the variables are contained|

### Fitting the model and displaying results

```{r}
# load the communities datasets
communities <- read.csv("http://uclspp.github.io/PUBLG100/data/communities.csv")
communities_employment <- read.csv("http://uclspp.github.io/PUBLG100/data/communities_employment.csv")
```

It seems that `state` and `communityname` are common to both datasets so we can use them to do the merge. 

Now let's use the [`merge()`](http://bit.ly/R_merge) function to merge these two datasets together. There are three arguments that we need to provide to the [`merge()`](http://bit.ly/R_merge) function so it knows what we are trying to merge and how. 

```
merge(x, y, by)
```

|Argument|Description|
|-|-|
|`x`|The first dataset to merge. This is the `communities` dataset in our case.|
|`y`|The second dataset to merge.  This is the `communities_employment` dataset. |
|`by`|Name of the column or columns that are common in both datasets. We know that `state` and `communityname` are common to both datasets so we'll pass them as a vector by combining the two names together.|

For more information on how the [`merge()`](http://bit.ly/R_merge) function works, type `help(merge)` in R.

```{r}
# merge the two datasets
communities <- merge(communities, communities_employment, by = c("state", "communityname"))

# explore dataset
names(communities)
```

```{r eval = FALSE}
View(communities)
```

Since our dataset has more columns than we need, let's select only the ones we're interested in and rename them with meaningful names. One approach would be to use either the [`subset()`](http://bit.ly/R_subset) function or the square bracket [`[ ]` extraction operator](http://bit.ly/R_extract) for selecting the columns we're interested in. But the easiest way to accomplish this is with the dplyr [`select()`](http://bit.ly/R_dplyr) function that allows us select the columns we need and rename them at the same time.

```{r}
communities <- select(communities, 
                      state, 
                      Community = communityname, 
                      UnemploymentRate = PctUnemployed, 
                      NoHighSchool = PctNotHSGrad,
                      White = racePctWhite)
```

Now that we've merged the dataset and renamed the columns the way we want, let's try to visualize the data. 

```{r}
plot(communities$NoHighSchool, communities$UnemploymentRate,
     xlab = "Adults without High School education (%)",
     ylab = "Unemployment Rate")
```

Now we can run the bivariate model:

```{r}
model1 <- lm(UnemploymentRate ~ NoHighSchool, data = communities)

summary(model1)
```

Now let's plot the regression line with our observations using the [`abline()`](http://bit.ly/R_abline) function.

```{r}
plot(communities$NoHighSchool, communities$UnemploymentRate,
     xlab = "Adults without High School education (%)",
     ylab = "Unemployment Rate")
abline(model1, col = "red")
```

Let's take a look at how to display the output of a regression model on the screen using the [`screenreg()`](http://bit.ly/R_texreg) function from `texreg`. 

```{r}
screenreg(model1)
```

Now, let's fit a multiple regression model and use `screenreg()` to display both models side by side. We can also use `htmlreg()` to create a Word file with the models:

```{r}
model2 <- lm(UnemploymentRate ~ NoHighSchool + White, data = communities)
summary(model2)
screenreg(list(model1, model2))
htmlreg(list(model1, model2), file="models.doc")
```

## Plotting confidence intervals of linear models

We will use the `ggplot2` package to plot the predicted values of the linear model. `ggplot2` works on the basis of layers. We first create the base layer that contains the data, and then we add the other layers using `+`

```{r}
g <- ggplot(data = communities, aes(y = UnemploymentRate, x = NoHighSchool))
g + geom_smooth(method = "lm")
g + geom_point() + geom_smooth(method = "lm") + 
  labs(title = "Model1", x = "Not on High School", y = "Unemployment Rate")

```

This option is limited to bivariate regressions and not to multiple models. The `Zelig` package allows us to estimate the confidence intervals for multiple models. This package requires that we re-estimate the models using their own function (`zelig()`), which is very similar to `lm()` but it also works for other regression models. We then need to define the values of X that we want to plot, and then we can simulate in order to get the confidence intervals

```{r}
z.out <- zelig(UnemploymentRate ~ NoHighSchool + White, data = communities, 
               model = "ls", cite=FALSE)
summary(z.out)

x.out <- setx(z.out, White = seq(0, 1, 0.1))
s.out <- sim(z.out, x=x.out)
summary(s.out)
plot(s.out, main = "Model 2")
```



## Heteroskedasticity

To test for heteroskedasticity we use the Breusch-Pagan test, with the `bptest()` function from the `lmtest` package:

```{r}
bptest(model2)

vcov(model2) # This function displays the variance-covariance matrix from model1
```

In order to recalculate the standard errors, we use the `coeftest()` function from the `lmtest` package to display the coefficients and their t-tests. One option of this function is that we can replace the variance-covariance matrix with a different (corrected) one. We will use the `vcovHC()` function from the `sandwich` package to estimate the new matrix.

```{r}

coeftest(model2) # Shows the coefficients and their corresponding t-tests

coeftest(model2, vcov=vcovHC(model2))

```


### Interactions

We will use the Quality of Government data using four variables:

|Variable| What is measures |
|-|-|
|`undp_hdi` | Human development index. Higher values, better quality of life.|
|`wbgi_cce` | Control of corruption. Higher values, better control of corruption. |
|`former_col` | A former colony or not. 1 indicates former colonies. |
|`wdi_gdpc`| GDP/captia in $US. Larger values more average income. |


```{r}
rm(list=ls()) # To clean our environment

# load quality of government institute 2015 dataset
world.data <- read.dta("http://uclspp.github.io/PUBLG100/data/QoG2012.dta")

# we remove NA's
world.data <- na.omit(world.data)

# let's transform the former_col variable into a factor
world.data$former_col <- factor(world.data$former_col, levels=c(0,1), labels = c("No", "Yes"))

# run the multiple regression
m1 <- lm(undp_hdi ~ wbgi_cce + former_col, data = world.data)

# regression table
screenreg(m1)
```

Now let's run the interaction term:


```{r}

m2 <- lm(undp_hdi ~ wbgi_cce * former_col, data = world.data)
summary(m2)
# F-test 
anova(m1, m2) 

# regression table
screenreg(list(m1, m2))
```

Plotting the interaction terms:

```{r, warning=FALSE}
# Using the plot function with the effects package
plot(effect(term= "wbgi_cce:former_col", mod=m2, x.var = "wbgi_cce"), multiline = TRUE)

# Using ggplot2
g <- ggplot(world.data, aes(x = wbgi_cce, y = undp_hdi, group = former_col, colour = former_col))
g + geom_smooth(method="lm")

# Using Zelig
z.out <- zelig(undp_hdi ~ wbgi_cce * former_col, data = world.data, model="ls", cite=FALSE)
# set covariates for countries that weren't colonised
x.out1 <- setx(z.out, former_col = "No", wbgi_cce = -3:2)
# set covariates for colonised countries
x.out2 <- setx(z.out, former_col = "Yes", wbgi_cce = -3:2)
# simulate 
s.out <- sim(z.out, x = x.out1, x1 = x.out2)
summary(s.out)
plot(s.out)
```

### Interactions: two continuous variables


```{r}

# plot of relationship b/w income & the human development index
plot( undp_hdi ~ wdi_gdpc,
      data = world.data,
      xlim = c( xmin = 0, xmax = 65000),
      ylim = c( ymin = 0, ymax = 1),
      frame = FALSE,
      xlab = "World Bank GDP/captia",
      ylab = "Human Development Index",
      main = "Relationship b/w Income and Quality of Life")

# add the regression line 
abline(lm(undp_hdi ~ wdi_gdpc, data = world.data))

# lowess line
lines(lowess(world.data$wdi_gdpc, world.data$undp_hdi), col="red")
```

To estimate the square effect, we need to use the `I()` index option

```{r}
# we include a quadradic term for income
m3 <- lm(undp_hdi ~ wdi_gdpc + I(wdi_gdpc^2), 
               data = world.data)

# regression output
summary(m3)
```

We can plot the quadratic relationship

```{r}
# Easiest way is with Zelig
z.out <- zelig(undp_hdi ~ wdi_gdpc + I(wdi_gdpc^2), 
               data = world.data, model = "ls", cite = F)

# setting covariates; GDP/captia is a sequence from 0 to 45000 by steps of 1000
x.out <- setx(z.out, wdi_gdpc = seq(0, 60000, 1000))

# simulate using our model and our covariates
s.out <- sim(z.out, x = x.out)

# plot the results
plot(s.out)
```

### Dplyr arrange()
On some occasions you might want to order a data set according to several criteria. `arrange()` from dplyr let's you do that. The default order is ascending. To change to descending, use `desc()`.

```{r}
# we order by ex-colony and then hdi 
head(arrange(world.data, former_col, undp_hdi))
# note: to change the data set you would have to assign it:
# world.data <- arrange(world.data, former_col, undp_hdi)

# the default order is ascending, for descending use:
head(arrange(world.data, desc(former_col, undp_hdi)))
```
